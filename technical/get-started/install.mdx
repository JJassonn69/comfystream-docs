---
title: "Install ComfyStream"
description: "A quick start guide to running your first AI video workflow with ComfyStream"
icon: "download"
---

ComfyStream is available as a docker image at [livepeer/comfystream:stable](https://hub.docker.com/r/livepeer/comfystream/tags?name=stable). An NVIDIA GPU is required to run ComfyStream. 

Choose a platform to install ComfyStream:
<CardGroup cols={2}>
  <Card title="Local GPU" icon="square-1" href="#local-gpu">
    Run ComfyStream and ComfyUI on Linux or Windows with a local GPU
  </Card>
  <Card title="Remote GPU" icon="square-2" href="#remote-gpu">
    Deploy and run ComfyStream and ComfyUI on a any remote server with a rented GPU
  </Card>
</CardGroup>

<h2>Local GPU</h2>

### Prerequisites

First, install the required system software:
<AccordionGroup>
  <Accordion title="Linux">
    - [Docker Engine](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)
    - [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-12-6-3-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu)
    - [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
  </Accordion>

  <Accordion title="Windows">
    1. Install WSL 2:
      - From a new command prompt, run: `wsl --install`
      - Update WSL if needed: `wsl.exe --update`
    2. Launch a new Terminal, then open a tab for **Ubuntu (WSL)**.
    3. Inside of WSL, install [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-12-6-3-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local) for WSL.
    4. Install [Docker Desktop for Windows](https://docs.docker.com/get-started/introduction/get-docker-desktop/) for Windows.
    5. Ensure WSL 2 Engine is enabled in Docker Desktop settings.
    6. Inside of WSL, run `docker ps` to verify you have access to docker. 
    7. From the WSL Terminal, continue with the steps below to setup the local environment
  </Accordion>
</AccordionGroup>

### Setting Up Local Environment 

1. Create directories for models and output:

<CodeGroup>
```bash Linux
mkdir -p ~/models/ComfyUI--models ~/models/ComfyUI--output  
```
```batch Command Line
mkdir %USERPROFILE%\models\ComfyUI--models %USERPROFILE%\models\ComfyUI--output
```
```powershell PowerShell
New-Item -ItemType Directory -Path "$env:USERPROFILE\models\ComfyUI--models", "$env:USERPROFILE\models\ComfyUI--output"
```
</CodeGroup>

2. Pull and run the container:

```bash
docker pull livepeer/comfystream:stable
```

<Info>
If using Windows, ensure Docker Desktop is running first
</Info>

<CodeGroup>
```bash Linux/WSL
docker run -it --gpus all \
-p 8188:8188 \
-p 8889:8889 \
-p 5678:5678 \
-v ~/models/ComfyUI--models:/workspace/ComfyUI/models \
-v ~/models/ComfyUI--output:/workspace/ComfyUI/output \
livepeer/comfystream:stable --download-models --build-engines --server
```
</CodeGroup>

<Note>
The `--download-models` and `--build-engines` flags will download required models and build TensorRT engines. This process may take some time depending on your network connection and GPU hardware and is only needed on the first run or when adding new models.
</Note>

<h2 id="remote-gpu">Remote GPU</h2>

ComfyStream can be deployed on any cloud provider with GPU support. Documentation includes multiple options for remote deployment:

<AccordionGroup>
  <Accordion title="Deploy using Ansible Playbooks (Cloud-Agnostic)">
    Ansible Playbooks offer a repeatable, reusable deployment system for deploying ComfyStream on any cloud provider. The included playbook automates the entire setup process.

    ### Prerequisites

    1. **Create a VM**:  
       Deploy a VM on a cloud provider such as TensorDock, AWS, Google Cloud, or Azure with the following minimum specifications:
       - **GPU**: 20GB VRAM  
       - **RAM**: 16GB  
       - **CPU**: 4 vCPUs  
       - **Storage**: 100GB

    2. **Open Required Ports**:  
       Ensure the following ports are open **inbound and outbound** on the VM's firewall/security group:
       - **SSH (Port 22)** – Remote access  
       - **HTTPS (Port 8189)** – ComfyStream access  
      
    3. **Install Ansible**:  
       Follow the [official Ansible installation guide](https://docs.ansible.com/ansible/latest/installation_guide/index.html).

    ### Deployment Steps

    1. **Configure the Inventory File**:
       Add the VM's public IP to [ansible/inventory.yml](https://github.com/livepeer/comfystream/blob/main/scripts/ansible/inventory.yaml).

    2. **Change the ComfyUI Password:**  
       Open the [ansible/plays/setup_comfystream.yaml](https://github.com/livepeer/comfystream/blob/main/scripts/ansible/plays/setup_comfystream.yaml) file and replace the `comfyui_password` value with your own secure password.

    3. **Run the Playbook**:  
       Execute:

       ```bash
       ansible-playbook -i ansible/inventory.yaml ansible/plays/setup_comfystream.yaml
       ```

       > When using a non-sudo user, add `--ask-become-pass` to provide the sudo password.

    4. **Access the Server**:  
       After the playbook completes, **ComfyStream** will start, and you can access **ComfyUI** at `https://<VM_IP>:<PORT_FOR_8189>`. 

       To persist the password, set the `comfyui_password` variable when running the playbook:

       ```bash
       ansible-playbook -i ansible/inventory.yaml ansible/plays/setup_comfystream.yaml -e "comfyui_password=YourSecurePasswordHere"
       ```

    <Note>
    If you encounter a `toomanyrequests` error while pulling the Docker image, either wait a few minutes or provide your Docker credentials when running the playbook:  

    ```bash
    ansible-playbook -i ansible/inventory.yaml ansible/plays/setup_comfystream.yaml -e "docker_hub_username=your_dockerhub_username docker_hub_password=your_dockerhub_pat"
    ```

    The ComfyStream Docker image is approximately 20GB. To check download progress, SSH into the VM and run:

    ```bash
    docker pull livepeer/comfystream:latest
    ```
    </Note>
  </Accordion>

  <Accordion title="Deploy on TensorDock (Fully Automated)">
    TensorDock provides affordable GPU instances perfect for running ComfyStream. We offer a fully automated script that handles VM provisioning, setup, and server launch.

    ### Prerequisites

    1. **Create a TensorDock Account**: 
       Sign up at [Tensordock](https://dashboard.tensordock.com/signup), add a payment method, and generate API credentials.

    2. **Set Up a Python Environment**:
       Create and activate a virtual environment with [Conda](https://docs.anaconda.com/miniconda/) and install the required dependencies:

       ```bash
       conda create -n comfystream python=3.8
       conda activate comfystream
       pip install -r requirements.txt
       ```

    ### Deployment Steps

    1. **Run the Script**:  
       Execute the following command to provision a VM and set up ComfyStream automatically:

       ```bash
       python spinup_comfystream_tensordock.py --api-key <API_KEY> --api-token <API_TOKEN>
       ```

    2. **Access the Server**:  
       Once the setup is complete, the script will display the URLs to access ComfyStream.

    3. **Stop & Delete the VM** (When No Longer Needed):
       To stop and remove the instance, run:

       ```bash
       python spinup_comfystream_tensordock.py --delete <VM_ID>
       ```

       Replace `<VM_ID>` with the VM ID found in the script logs or the [TensorDock dashboard](https://dashboard.tensordock.com/instances).

    <Note>
    If you encounter `max retries exceeded with URL` errors, the VM might have been created but is inaccessible.  
    Check the [TensorDock dashboard](https://dashboard.tensordock.com/instances), delete the VM manually, wait 2-3 minutes, then rerun the script.
    </Note>
  </Accordion>

  <Accordion title="Deploy on RunPod">
    1. Deploy using the RunPod template 
    [livepeer-comfystream](https://runpod.io/console/deploy?template=w01m180vxx&ref=u8tlskew)
    <Note>
    First-time deployment to a network volume takes approximately 20-45 minutes depending on pod performance. 
    For faster deployment without data persistence, use the RunPod template [livepeer-comfystream-novolume](https://runpod.io/console/deploy?template=j4p1g7t5vs&ref=u8tlskew)
    </Note>
    2. When using the [livepeer-comfystream](https://runpod.io/console/deploy?template=w01m180vxx&ref=u8tlskew) template, create and select a network volume to persist files.
    ![New Network Volume](../../images/networkvolume.png)
    3. Select an appropriate GPU (ex: RTX 4090) and click **Deploy On-Demand**
    4. Click the **Logs** button to monitor deployment progress.
    ![runpod-log](../../images/runpod-log.png)
    5. Once the container is fully running, you can access ComfyUI by clicking **Connect**
  </Accordion>
</AccordionGroup>


## Accessing ComfyUI
<AccordionGroup>
  <Accordion title="RunPod">
  1. Click **Connect** in pod dashboard
    ![comfyui-connect-pod](../../images/comfyui-connect-pod.png)
  2. Access ComfyUI: **HTTP Service -> :8188**
  3. ComfyUI will open in a new tab
  </Accordion>

  <Accordion title="Local">
  When running the docker image locally, you can access ComfyUI at [http://localhost:8188](http://localhost:8188).
  </Accordion>
</AccordionGroup>

## Accessing ComfyStream 
1. From ComfyUI, click the **ComfyStream** menu button:
![comfystream-node-ui.png](../../images/comfystream-node-ui.png)

2. Click **Server Settings** to verify ComfyStream is configured to bind to the correct interface and port <Note>When deploying to remote environments like RunPod, you will need to set the Host to `0.0.0.0`. </Note>
![comfystream-server-settings.png](../../images/comfystream-server-settings.png)
3. Click **Save**.
4. Open the **ComfyStream** menu again, then click **Start ComfyStream Server**. Wait for the server status indicator to turn green. 
<Note>
You can also monitor ComfyStream server logs in ComfyUI's log terminal tab.
</Note>
5. Click **Open ComfyStream UI** to launch in a new tab.
Alternatively, you can double-click the node graph and search for **ComfyStream UI Preview**
![comfystream-ui-embed.png](../../images/comfystream-ui-embed.png)
